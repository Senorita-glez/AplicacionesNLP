{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import os\\nimport csv\\n\\n# Ruta de la carpeta que contiene los archivos .txt\\ndirectorio_txt = \\'./dataInUse\\'\\n\\n# Ruta del archivo CSV de salida\\narchivo_csv = \\'recolectedLyrics.csv\\'\\n\\n# Abrir el archivo CSV en modo escritura\\nwith open(archivo_csv, mode=\\'w\\', newline=\\'\\', encoding=\\'utf-8\\') as csvfile:\\n    writer = csv.writer(csvfile)\\n    \\n    # Escribir el encabezado del CSV\\n    writer.writerow([\\'Artist\\', \\'Lyrics\\'])\\n    \\n    # Recorrer cada archivo .txt en el directorio\\n    for archivo in os.listdir(directorio_txt):\\n        if archivo.endswith(\\'.txt\\'):\\n            # Obtener la ruta completa del archivo .txt\\n            ruta_archivo = os.path.join(directorio_txt, archivo)\\n            nombre_sin_extension = os.path.splitext(archivo)[0]\\n            # Abrir el archivo .txt en modo lectura\\n            with open(ruta_archivo, mode=\\'r\\', encoding=\\'utf-8\\') as file:\\n                # Leer cada línea del archivo .txt\\n                for linea in file:\\n                    # Escribir el nombre del archivo y la línea en el CSV\\n                    writer.writerow([nombre_sin_extension, linea.strip()])  # .strip() para quitar saltos de línea extra\\n\\nprint(\"El archivo CSV ha sido creado exitosamente.\")\\n\\n# Ruta del archivo CSV original y el archivo de salida limpio\\narchivo_csv_original = \\'recolectedLyrics.csv\\'\\narchivo_csv_limpio = \\'recolectedLyricsClean.csv\\'\\n\\n# Abrir el archivo CSV original para leer\\nwith open(archivo_csv_original, mode=\\'r\\', encoding=\\'utf-8\\') as infile:\\n    reader = csv.reader(infile)\\n    \\n    # Leer todas las filas del CSV\\n    filas = list(reader)\\n\\n# Filtrar las filas según las condiciones:\\n# - No tenga solo espacios en blanco.\\n# - Tenga más de 4 palabras.\\nfilas_limpias = []\\nfor fila in filas:\\n    if len(fila) > 1:  # Asegurarse de que la fila tenga más de una columna\\n        nombre_archivo, contenido_linea = fila\\n        if contenido_linea.strip() != \\'\\' and len(contenido_linea.split()) > 4:\\n            filas_limpias.append(fila)\\n\\n# Escribir las filas filtradas en un nuevo archivo CSV\\nwith open(archivo_csv_limpio, mode=\\'w\\', newline=\\'\\', encoding=\\'utf-8\\') as outfile:\\n    writer = csv.writer(outfile)\\n    \\n    # Escribir el encabezado\\n    writer.writerow([\\'Artist\\', \\'Lyrics\\'])\\n    \\n    # Escribir las filas filtradas\\n    writer.writerows(filas_limpias)\\n\\nprint(f\"El archivo CSV limpio ha sido creado exitosamente en \\'{archivo_csv_limpio}\\'.\")\\n\\nimport csv\\nfrom collections import defaultdict\\nimport random\\n\\n# Ruta del archivo CSV limpio y el archivo de salida balanceado\\narchivo_csv_limpio = \\'recolectedLyricsClean.csv\\'\\narchivo_csv_balanceado = \\'recolectedLyricsCleanBalanced.csv\\'\\n\\n# Paso 1: Leer el archivo CSV limpio y contar la frecuencia de cada nombre de archivo\\narchivos_contenido = defaultdict(list)\\n\\n# Leer el archivo CSV limpio\\nwith open(archivo_csv_limpio, mode=\\'r\\', encoding=\\'utf-8\\') as infile:\\n    reader = csv.reader(infile)\\n    next(reader)  # Salta el encabezado\\n    for fila in reader:\\n        nombre_archivo, contenido_linea = fila\\n        archivos_contenido[nombre_archivo].append(contenido_linea)\\n\\n# Paso 2: Determinar el número mínimo de filas entre los diferentes archivos\\nmin_filas = min(len(contenido) for contenido in archivos_contenido.values())\\n\\n# Paso 3: Crear una lista de filas balanceadas\\nfilas_balanceadas = []\\nfor nombre_archivo, contenido_lineas in archivos_contenido.items():\\n    # Seleccionamos una cantidad de líneas igual al número mínimo de filas\\n    filas_balanceadas.extend([(nombre_archivo, linea) for linea in random.sample(contenido_lineas, min_filas)])\\n\\n# Paso 4: Escribir el archivo CSV balanceado\\nwith open(archivo_csv_balanceado, mode=\\'w\\', newline=\\'\\', encoding=\\'utf-8\\') as outfile:\\n    writer = csv.writer(outfile)\\n    # Escribir el encabezado\\n    writer.writerow([\\'Artist\\', \\'Lyrics\\'])\\n    # Escribir las filas balanceadas\\n    writer.writerows(filas_balanceadas)\\n\\nprint(f\"El archivo CSV balanceado ha sido creado exitosamente en \\'{archivo_csv_balanceado}\\'.\")\\n\\n'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''import os\n",
    "import csv\n",
    "\n",
    "# Ruta de la carpeta que contiene los archivos .txt\n",
    "directorio_txt = './dataInUse'\n",
    "\n",
    "# Ruta del archivo CSV de salida\n",
    "archivo_csv = 'recolectedLyrics.csv'\n",
    "\n",
    "# Abrir el archivo CSV en modo escritura\n",
    "with open(archivo_csv, mode='w', newline='', encoding='utf-8') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    \n",
    "    # Escribir el encabezado del CSV\n",
    "    writer.writerow(['Artist', 'Lyrics'])\n",
    "    \n",
    "    # Recorrer cada archivo .txt en el directorio\n",
    "    for archivo in os.listdir(directorio_txt):\n",
    "        if archivo.endswith('.txt'):\n",
    "            # Obtener la ruta completa del archivo .txt\n",
    "            ruta_archivo = os.path.join(directorio_txt, archivo)\n",
    "            nombre_sin_extension = os.path.splitext(archivo)[0]\n",
    "            # Abrir el archivo .txt en modo lectura\n",
    "            with open(ruta_archivo, mode='r', encoding='utf-8') as file:\n",
    "                # Leer cada línea del archivo .txt\n",
    "                for linea in file:\n",
    "                    # Escribir el nombre del archivo y la línea en el CSV\n",
    "                    writer.writerow([nombre_sin_extension, linea.strip()])  # .strip() para quitar saltos de línea extra\n",
    "\n",
    "print(\"El archivo CSV ha sido creado exitosamente.\")\n",
    "\n",
    "# Ruta del archivo CSV original y el archivo de salida limpio\n",
    "archivo_csv_original = 'recolectedLyrics.csv'\n",
    "archivo_csv_limpio = 'recolectedLyricsClean.csv'\n",
    "\n",
    "# Abrir el archivo CSV original para leer\n",
    "with open(archivo_csv_original, mode='r', encoding='utf-8') as infile:\n",
    "    reader = csv.reader(infile)\n",
    "    \n",
    "    # Leer todas las filas del CSV\n",
    "    filas = list(reader)\n",
    "\n",
    "# Filtrar las filas según las condiciones:\n",
    "# - No tenga solo espacios en blanco.\n",
    "# - Tenga más de 4 palabras.\n",
    "filas_limpias = []\n",
    "for fila in filas:\n",
    "    if len(fila) > 1:  # Asegurarse de que la fila tenga más de una columna\n",
    "        nombre_archivo, contenido_linea = fila\n",
    "        if contenido_linea.strip() != '' and len(contenido_linea.split()) > 4:\n",
    "            filas_limpias.append(fila)\n",
    "\n",
    "# Escribir las filas filtradas en un nuevo archivo CSV\n",
    "with open(archivo_csv_limpio, mode='w', newline='', encoding='utf-8') as outfile:\n",
    "    writer = csv.writer(outfile)\n",
    "    \n",
    "    # Escribir el encabezado\n",
    "    writer.writerow(['Artist', 'Lyrics'])\n",
    "    \n",
    "    # Escribir las filas filtradas\n",
    "    writer.writerows(filas_limpias)\n",
    "\n",
    "print(f\"El archivo CSV limpio ha sido creado exitosamente en '{archivo_csv_limpio}'.\")\n",
    "\n",
    "import csv\n",
    "from collections import defaultdict\n",
    "import random\n",
    "\n",
    "# Ruta del archivo CSV limpio y el archivo de salida balanceado\n",
    "archivo_csv_limpio = 'recolectedLyricsClean.csv'\n",
    "archivo_csv_balanceado = 'recolectedLyricsCleanBalanced.csv'\n",
    "\n",
    "# Paso 1: Leer el archivo CSV limpio y contar la frecuencia de cada nombre de archivo\n",
    "archivos_contenido = defaultdict(list)\n",
    "\n",
    "# Leer el archivo CSV limpio\n",
    "with open(archivo_csv_limpio, mode='r', encoding='utf-8') as infile:\n",
    "    reader = csv.reader(infile)\n",
    "    next(reader)  # Salta el encabezado\n",
    "    for fila in reader:\n",
    "        nombre_archivo, contenido_linea = fila\n",
    "        archivos_contenido[nombre_archivo].append(contenido_linea)\n",
    "\n",
    "# Paso 2: Determinar el número mínimo de filas entre los diferentes archivos\n",
    "min_filas = min(len(contenido) for contenido in archivos_contenido.values())\n",
    "\n",
    "# Paso 3: Crear una lista de filas balanceadas\n",
    "filas_balanceadas = []\n",
    "for nombre_archivo, contenido_lineas in archivos_contenido.items():\n",
    "    # Seleccionamos una cantidad de líneas igual al número mínimo de filas\n",
    "    filas_balanceadas.extend([(nombre_archivo, linea) for linea in random.sample(contenido_lineas, min_filas)])\n",
    "\n",
    "# Paso 4: Escribir el archivo CSV balanceado\n",
    "with open(archivo_csv_balanceado, mode='w', newline='', encoding='utf-8') as outfile:\n",
    "    writer = csv.writer(outfile)\n",
    "    # Escribir el encabezado\n",
    "    writer.writerow(['Artist', 'Lyrics'])\n",
    "    # Escribir las filas balanceadas\n",
    "    writer.writerows(filas_balanceadas)\n",
    "\n",
    "print(f\"El archivo CSV balanceado ha sido creado exitosamente en '{archivo_csv_balanceado}'.\")\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El archivo CSV filtrado ha sido creado exitosamente.\n",
      "El archivo CSV balanceado y con las filas unidas ha sido creado exitosamente.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "# Ruta del archivo CSV original\n",
    "archivo_csv = 'recolectedLyrics.csv'\n",
    "\n",
    "# Ruta del archivo CSV filtrado\n",
    "archivo_csv_filtrado = 'recolectedLyricsClean0.csv'\n",
    "\n",
    "# Abrir el archivo CSV original y el nuevo archivo para escribir los datos filtrados\n",
    "with open(archivo_csv, mode='r', encoding='utf-8') as csvfile, \\\n",
    "     open(archivo_csv_filtrado, mode='w', newline='', encoding='utf-8') as csvfile_filtrado:\n",
    "    \n",
    "    reader = csv.reader(csvfile)\n",
    "    writer = csv.writer(csvfile_filtrado)\n",
    "    \n",
    "    # Escribir el encabezado en el archivo filtrado\n",
    "    encabezado = next(reader)\n",
    "    writer.writerow(encabezado)\n",
    "    \n",
    "    # Filtrar y escribir solo las filas válidas\n",
    "    for fila in reader:\n",
    "        contenido_linea = fila[1].strip()  # Obtener el contenido de la línea (columna 1)\n",
    "        \n",
    "        # Contar el número de palabras en el contenido\n",
    "        if contenido_linea != '':\n",
    "            writer.writerow(fila)\n",
    "\n",
    "print(\"El archivo CSV filtrado ha sido creado exitosamente.\")\n",
    "\n",
    "\n",
    "import csv\n",
    "from collections import defaultdict\n",
    "\n",
    "# Ruta del archivo CSV original\n",
    "archivo_csv = 'recolectedLyricsClean0.csv'\n",
    "\n",
    "# Ruta del archivo CSV con las filas unidas\n",
    "archivo_csv_final = 'recolectedLyricsClean2Balanced15.csv'\n",
    "\n",
    "# Leer el archivo CSV original y agrupar por 'Artist'\n",
    "\n",
    "# Leer el archivo CSV y agrupar las filas por artista\n",
    "artistas_filas = defaultdict(list)\n",
    "\n",
    "with open(archivo_csv, mode='r', encoding='utf-8') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)  # Leer el archivo con encabezados\n",
    "    for fila in reader:\n",
    "        artistas_filas[fila['Artist']].append(fila)  # Agrupar por el valor en la columna Artist\n",
    "\n",
    "# Encontrar el mínimo número de filas entre los artistas\n",
    "minimo_filas = min(len(filas) for filas in artistas_filas.values())\n",
    "\n",
    "# Crear un nuevo archivo CSV con las filas balanceadas\n",
    "with open(archivo_csv_final, mode='w', newline='', encoding='utf-8') as csvfile_final:\n",
    "    # Obtener los encabezados\n",
    "    encabezados = ['Artist', 'Lyrics']\n",
    "    writer = csv.DictWriter(csvfile_final, fieldnames=encabezados)\n",
    "    writer.writeheader()  # Escribir el encabezado\n",
    "\n",
    "    # Para cada artista, tomar las filas necesarias y unir cada 4 en una fila\n",
    "    for artista, filas in artistas_filas.items():\n",
    "        filas_balanceadas = filas[:minimo_filas]  # Limitar al número mínimo de filas\n",
    "        for i in range(0, len(filas_balanceadas), 15):  # Procesar de 4 en 4\n",
    "            # Unir las letras (lyrics) de las 4 filas\n",
    "            lyrics_unidos = \" \".join(fila['Lyrics'] for fila in filas_balanceadas[i:i+15])\n",
    "            writer.writerow({'Artist': artista, 'Lyrics': lyrics_unidos})\n",
    "\n",
    "print(\"El archivo CSV balanceado y con las filas unidas ha sido creado exitosamente.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nltk'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdill\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mglob\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01mcsv\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstem\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordNetLemmatizer \n\u001b[1;32m      8\u001b[0m nltk\u001b[38;5;241m.\u001b[39mdownload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwordnet\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'nltk'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import dill\n",
    "import glob, csv\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt') # Download this as this allows you to tokenize words in a string.\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [Errno -3]\n",
      "[nltk_data]     Temporary failure in name resolution>\n",
      "[nltk_data] Error loading punkt: <urlopen error [Errno -3] Temporary\n",
      "[nltk_data]     failure in name resolution>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisión SVM (Histogramas): 9.59%\n",
      "\n",
      "Reporte de Clasificación SVM (Histogramas):\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "         blink-182       0.00      0.00      0.00        13\n",
      "    britney-spears       0.05      0.07      0.06        15\n",
      "           rihanna       0.00      0.00      0.00        17\n",
      "         lady-gaga       0.00      0.00      0.00        11\n",
      "   michael-jackson       0.00      0.00      0.00        15\n",
      "           nirvana       0.47      0.41      0.44        17\n",
      "        bruno-mars       0.16      0.19      0.17        16\n",
      "         radiohead       0.00      0.00      0.00        12\n",
      "             adele       0.00      0.00      0.00        19\n",
      "lin-manuel-miranda       0.23      0.27      0.25        11\n",
      "\n",
      "          accuracy                           0.10       146\n",
      "         macro avg       0.09      0.09      0.09       146\n",
      "      weighted avg       0.09      0.10      0.09       146\n",
      "\n",
      "Precisión SVM (TF-IDF): 19.18%\n",
      "\n",
      "Reporte de Clasificación SVM (TF-IDF):\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "         blink-182       0.00      0.00      0.00        13\n",
      "    britney-spears       0.00      0.00      0.00        15\n",
      "           rihanna       0.00      0.00      0.00        17\n",
      "         lady-gaga       0.00      0.00      0.00        11\n",
      "   michael-jackson       0.00      0.00      0.00        15\n",
      "           nirvana       0.94      0.94      0.94        17\n",
      "        bruno-mars       0.00      0.00      0.00        16\n",
      "         radiohead       0.13      0.17      0.15        12\n",
      "             adele       0.00      0.00      0.00        19\n",
      "lin-manuel-miranda       0.77      0.91      0.83        11\n",
      "\n",
      "          accuracy                           0.19       146\n",
      "         macro avg       0.18      0.20      0.19       146\n",
      "      weighted avg       0.18      0.19      0.18       146\n",
      "\n",
      "Precisión Random Forest (Histogramas): 11.64%\n",
      "\n",
      "Reporte de Clasificación Random Forest (Histogramas):\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "         blink-182       0.05      0.08      0.06        13\n",
      "    britney-spears       0.10      0.07      0.08        15\n",
      "           rihanna       0.09      0.06      0.07        17\n",
      "         lady-gaga       0.06      0.09      0.07        11\n",
      "   michael-jackson       0.00      0.00      0.00        15\n",
      "           nirvana       0.69      0.53      0.60        17\n",
      "        bruno-mars       0.05      0.06      0.05        16\n",
      "         radiohead       0.00      0.00      0.00        12\n",
      "             adele       0.00      0.00      0.00        19\n",
      "lin-manuel-miranda       0.33      0.27      0.30        11\n",
      "\n",
      "          accuracy                           0.12       146\n",
      "         macro avg       0.14      0.12      0.12       146\n",
      "      weighted avg       0.14      0.12      0.13       146\n",
      "\n",
      "Precisión Random Forest (TF-IDF): 15.75%\n",
      "\n",
      "Reporte de Clasificación Random Forest (TF-IDF):\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "         blink-182       0.00      0.00      0.00        13\n",
      "    britney-spears       0.00      0.00      0.00        15\n",
      "           rihanna       0.00      0.00      0.00        17\n",
      "         lady-gaga       0.00      0.00      0.00        11\n",
      "   michael-jackson       0.00      0.00      0.00        15\n",
      "           nirvana       0.87      0.76      0.81        17\n",
      "        bruno-mars       0.00      0.00      0.00        16\n",
      "         radiohead       0.08      0.08      0.08        12\n",
      "             adele       0.00      0.00      0.00        19\n",
      "lin-manuel-miranda       0.45      0.82      0.58        11\n",
      "\n",
      "          accuracy                           0.16       146\n",
      "         macro avg       0.14      0.17      0.15       146\n",
      "      weighted avg       0.14      0.16      0.15       146\n",
      "\n",
      "Precisión Multinomial Naive Bayes (TF-IDF): 18.49%\n",
      "\n",
      "Reporte de Clasificación Multinomial Naive Bayes (TF-IDF):\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "         blink-182       0.00      0.00      0.00        13\n",
      "    britney-spears       0.00      0.00      0.00        15\n",
      "           rihanna       0.00      0.00      0.00        17\n",
      "         lady-gaga       0.00      0.00      0.00        11\n",
      "   michael-jackson       0.00      0.00      0.00        15\n",
      "           nirvana       0.74      1.00      0.85        17\n",
      "        bruno-mars       0.00      0.00      0.00        16\n",
      "         radiohead       0.11      0.17      0.13        12\n",
      "             adele       0.00      0.00      0.00        19\n",
      "lin-manuel-miranda       0.89      0.73      0.80        11\n",
      "\n",
      "          accuracy                           0.18       146\n",
      "         macro avg       0.17      0.19      0.18       146\n",
      "      weighted avg       0.16      0.18      0.17       146\n",
      "\n",
      "Precisión SVM (Word2Vec): 15.07%\n",
      "\n",
      "Reporte de Clasificación SVM (Word2Vec):\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "         blink-182       0.00      0.00      0.00        13\n",
      "    britney-spears       0.00      0.00      0.00        15\n",
      "           rihanna       0.22      0.12      0.15        17\n",
      "         lady-gaga       0.00      0.00      0.00        11\n",
      "   michael-jackson       0.00      0.00      0.00        15\n",
      "           nirvana       0.00      0.00      0.00        17\n",
      "        bruno-mars       0.00      0.00      0.00        16\n",
      "         radiohead       0.00      0.00      0.00        12\n",
      "             adele       0.15      0.68      0.24        19\n",
      "lin-manuel-miranda       0.16      0.64      0.25        11\n",
      "\n",
      "          accuracy                           0.15       146\n",
      "         macro avg       0.05      0.14      0.06       146\n",
      "      weighted avg       0.06      0.15      0.07       146\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/senorita-glez/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/senorita-glez/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/senorita-glez/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisión Random Forest (Word2Vec): 16.44%\n",
      "\n",
      "Reporte de Clasificación Random Forest (Word2Vec):\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "         blink-182       0.06      0.08      0.06        13\n",
      "    britney-spears       0.00      0.00      0.00        15\n",
      "           rihanna       0.15      0.12      0.13        17\n",
      "         lady-gaga       0.20      0.18      0.19        11\n",
      "   michael-jackson       0.06      0.07      0.06        15\n",
      "           nirvana       0.50      0.47      0.48        17\n",
      "        bruno-mars       0.06      0.06      0.06        16\n",
      "         radiohead       0.18      0.25      0.21        12\n",
      "             adele       0.08      0.05      0.06        19\n",
      "lin-manuel-miranda       0.26      0.45      0.33        11\n",
      "\n",
      "          accuracy                           0.16       146\n",
      "         macro avg       0.16      0.17      0.16       146\n",
      "      weighted avg       0.15      0.16      0.16       146\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "import nltk\n",
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Descargar recursos necesarios\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Configurar las stopwords\n",
    "stop = set(stopwords.words('english'))\n",
    "\n",
    "# Archivos CSV\n",
    "csv_estrofa = \"recolectedLyricsClean2Balanced15.csv\"\n",
    "csv_line = \"recolectedLyricsCleanBalancedByLine.csv\"\n",
    "\n",
    "# Leer los datos\n",
    "authorEstrofa = pd.read_csv(csv_estrofa)\n",
    "authorLine = pd.read_csv(csv_line)\n",
    "\n",
    "# Dividir en conjuntos de entrenamiento y prueba (70-30)\n",
    "estrofa_train, estrofa_test = train_test_split(authorEstrofa, test_size=0.20, random_state=42)\n",
    "line_train, line_test = train_test_split(authorLine, test_size=0.20, random_state=42)\n",
    "\n",
    "# Procesar los DataFrames\n",
    "for df in [estrofa_train, estrofa_test, line_train, line_test]:\n",
    "    if 'Lyrics' in df.columns:\n",
    "        # Convertir a minúsculas, eliminar puntuaciones y tokenizar\n",
    "        df['Lyrics_Cleaned'] = df['Lyrics'].str.lower().str.replace('[^\\w\\s]', '', regex=True)\n",
    "        df['Lyrics_Tokenized'] = df['Lyrics_Cleaned'].apply(lambda x: word_tokenize(str(x)))\n",
    "        # Quitar stopwords\n",
    "        #df['Lyrics_Tokenized_NoStopwords'] = df['Lyrics_Tokenized'].apply(lambda x: [word for word in x if word not in stop])\n",
    "\n",
    "    if 'Artist' in df.columns:\n",
    "        # Mapear valores únicos de 'Artist' a números\n",
    "        unique_authors = df['Artist'].unique()\n",
    "        author_mapping = {author: idx for idx, author in enumerate(unique_authors)}\n",
    "        df['Artist_Mapped'] = df['Artist'].map(author_mapping)\n",
    "\n",
    "# Mostrar vistas previas de los DataFrames procesados\n",
    "'''print(\"Estrofa - Entrenamiento Procesado:\", estrofa_train.head())\n",
    "print(\"Estrofa - Prueba Procesado:\", estrofa_test.head())\n",
    "print(\"Línea - Entrenamiento Procesado:\", line_train.head())\n",
    "print(\"Línea - Prueba Procesado:\", line_test.head())'''\n",
    "\n",
    "\n",
    "def generate_char_histograms(text_series):\n",
    "    histograms = []\n",
    "    for text in text_series:\n",
    "        if isinstance(text, list):\n",
    "            text = ' '.join(text)\n",
    "        text = str(text)\n",
    "        char_counts = np.zeros(128)\n",
    "        for char in text:\n",
    "            if ord(char) < 128:\n",
    "                char_counts[ord(char)] += 1\n",
    "        total_chars = np.sum(char_counts)\n",
    "        if total_chars > 0:\n",
    "            char_counts /= total_chars\n",
    "        histograms.append(char_counts)\n",
    "    return np.array(histograms)\n",
    "\n",
    "# Generar histogramas para el conjunto de entrenamiento y prueba\n",
    "X_train_hist = generate_char_histograms(estrofa_train['Lyrics_Cleaned'])\n",
    "X_test_hist = generate_char_histograms(estrofa_test['Lyrics_Cleaned'])\n",
    "\n",
    "# Vectorización TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=2000)\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(estrofa_train['Lyrics_Cleaned']).toarray()\n",
    "X_test_tfidf = tfidf_vectorizer.transform(estrofa_test['Lyrics_Cleaned']).toarray()\n",
    "\n",
    "# Etiquetas\n",
    "y_train = estrofa_train['Artist_Mapped'].values\n",
    "y_test = estrofa_test['Artist_Mapped'].values\n",
    "\n",
    "# Escalado de características para histogramas\n",
    "scaler = StandardScaler()\n",
    "X_train_hist_scaled = scaler.fit_transform(X_train_hist)\n",
    "X_test_hist_scaled = scaler.transform(X_test_hist)\n",
    "\n",
    "# Entrenar y evaluar SVM con histogramas\n",
    "svm_model_hist = SVC(kernel='linear', random_state=42)\n",
    "svm_model_hist.fit(X_train_hist_scaled, y_train)\n",
    "y_pred_hist = svm_model_hist.predict(X_test_hist_scaled)\n",
    "print(f\"Precisión SVM (Histogramas): {accuracy_score(y_test, y_pred_hist) * 100:.2f}%\")\n",
    "print(\"\\nReporte de Clasificación SVM (Histogramas):\")\n",
    "print(classification_report(y_test, y_pred_hist, target_names=unique_authors))\n",
    "\n",
    "# Entrenar y evaluar SVM con TF-IDF\n",
    "svm_model_tfidf = SVC(kernel='linear', random_state=42)\n",
    "svm_model_tfidf.fit(X_train_tfidf, y_train)\n",
    "y_pred_tfidf = svm_model_tfidf.predict(X_test_tfidf)\n",
    "print(f\"Precisión SVM (TF-IDF): {accuracy_score(y_test, y_pred_tfidf) * 100:.2f}%\")\n",
    "print(\"\\nReporte de Clasificación SVM (TF-IDF):\")\n",
    "print(classification_report(y_test, y_pred_tfidf, target_names=unique_authors))\n",
    "\n",
    "# Entrenar y evaluar Random Forest con histogramas\n",
    "rf_model_hist = RandomForestClassifier(random_state=42, n_estimators=100)\n",
    "rf_model_hist.fit(X_train_hist_scaled, y_train)\n",
    "y_pred_rf_hist = rf_model_hist.predict(X_test_hist_scaled)\n",
    "print(f\"Precisión Random Forest (Histogramas): {accuracy_score(y_test, y_pred_rf_hist) * 100:.2f}%\")\n",
    "print(\"\\nReporte de Clasificación Random Forest (Histogramas):\")\n",
    "print(classification_report(y_test, y_pred_rf_hist, target_names=unique_authors))\n",
    "\n",
    "# Entrenar y evaluar Random Forest con TF-IDF\n",
    "rf_model_tfidf = RandomForestClassifier(random_state=42, n_estimators=100)\n",
    "rf_model_tfidf.fit(X_train_tfidf, y_train)\n",
    "y_pred_rf_tfidf = rf_model_tfidf.predict(X_test_tfidf)\n",
    "print(f\"Precisión Random Forest (TF-IDF): {accuracy_score(y_test, y_pred_rf_tfidf) * 100:.2f}%\")\n",
    "print(\"\\nReporte de Clasificación Random Forest (TF-IDF):\")\n",
    "print(classification_report(y_test, y_pred_rf_tfidf, target_names=unique_authors))\n",
    "\n",
    "# Entrenar y evaluar Multinomial Naive Bayes con TF-IDF\n",
    "nb_model_tfidf = MultinomialNB()\n",
    "nb_model_tfidf.fit(X_train_tfidf, y_train)\n",
    "y_pred_nb_tfidf = nb_model_tfidf.predict(X_test_tfidf)\n",
    "print(f\"Precisión Multinomial Naive Bayes (TF-IDF): {accuracy_score(y_test, y_pred_nb_tfidf) * 100:.2f}%\")\n",
    "print(\"\\nReporte de Clasificación Multinomial Naive Bayes (TF-IDF):\")\n",
    "print(classification_report(y_test, y_pred_nb_tfidf, target_names=unique_authors))\n",
    "\n",
    "w2v_model = Word2Vec(sentences=estrofa_train['Lyrics_Tokenized'], vector_size=500, min_count=5, workers=5)\n",
    "\n",
    "# Función para convertir textos a vectores promedio utilizando Word2Vec\n",
    "def text_to_w2v(text_tokens, w2v_model):\n",
    "    vector = np.zeros(w2v_model.vector_size)\n",
    "    count = 0\n",
    "    for word in text_tokens:\n",
    "        if word in w2v_model.wv:\n",
    "            vector += w2v_model.wv[word]\n",
    "            count += 1\n",
    "    if count > 0:\n",
    "        vector /= count\n",
    "    return vector\n",
    "\n",
    "# Convertir los textos de entrenamiento y prueba a vectores promedio\n",
    "X_train_w2v = np.array([text_to_w2v(tokens, w2v_model) for tokens in estrofa_train['Lyrics_Tokenized']])\n",
    "X_test_w2v = np.array([text_to_w2v(tokens, w2v_model) for tokens in estrofa_test['Lyrics_Tokenized']])\n",
    "\n",
    "# Etiquetas\n",
    "y_train = estrofa_train['Artist_Mapped'].values\n",
    "y_test = estrofa_test['Artist_Mapped'].values\n",
    "\n",
    "# Entrenar y evaluar SVM con Word2Vec\n",
    "svm_model_w2v = SVC(kernel='linear', random_state=42)\n",
    "svm_model_w2v.fit(X_train_w2v, y_train)\n",
    "y_pred_w2v = svm_model_w2v.predict(X_test_w2v)\n",
    "print(f\"Precisión SVM (Word2Vec): {accuracy_score(y_test, y_pred_w2v) * 100:.2f}%\")\n",
    "print(\"\\nReporte de Clasificación SVM (Word2Vec):\")\n",
    "print(classification_report(y_test, y_pred_w2v, target_names=unique_authors))\n",
    "\n",
    "# Entrenar y evaluar Random Forest con Word2Vec\n",
    "rf_model_w2v = RandomForestClassifier(random_state=42, n_estimators=150)\n",
    "rf_model_w2v.fit(X_train_w2v, y_train)\n",
    "y_pred_rf_w2v = rf_model_w2v.predict(X_test_w2v)\n",
    "print(f\"Precisión Random Forest (Word2Vec): {accuracy_score(y_test, y_pred_rf_w2v) * 100:.2f}%\")\n",
    "print(\"\\nReporte de Clasificación Random Forest (Word2Vec):\")\n",
    "print(classification_report(y_test, y_pred_rf_w2v, target_names=unique_authors))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Article level\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.0.2 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"/usr/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/usr/lib/python3.9/runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/senorita-glez/torch_env/lib/python3.9/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/home/senorita-glez/torch_env/lib/python3.9/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/home/senorita-glez/torch_env/lib/python3.9/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/home/senorita-glez/torch_env/lib/python3.9/site-packages/tornado/platform/asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n",
      "    handle._run()\n",
      "  File \"/usr/lib/python3.9/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/home/senorita-glez/torch_env/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/home/senorita-glez/torch_env/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/home/senorita-glez/torch_env/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"/home/senorita-glez/torch_env/lib/python3.9/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/home/senorita-glez/torch_env/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/home/senorita-glez/torch_env/lib/python3.9/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/home/senorita-glez/torch_env/lib/python3.9/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/home/senorita-glez/torch_env/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3048, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/home/senorita-glez/torch_env/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3103, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/home/senorita-glez/torch_env/lib/python3.9/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/home/senorita-glez/torch_env/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3308, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/home/senorita-glez/torch_env/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3490, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/home/senorita-glez/torch_env/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3550, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_22531/567279997.py\", line 4, in <module>\n",
      "    from torchtext.legacy import data\n",
      "  File \"/home/senorita-glez/torch_env/lib/python3.9/site-packages/torchtext/__init__.py\", line 1, in <module>\n",
      "    from . import data\n",
      "  File \"/home/senorita-glez/torch_env/lib/python3.9/site-packages/torchtext/data/__init__.py\", line 1, in <module>\n",
      "    from .metrics import bleu_score\n",
      "  File \"/home/senorita-glez/torch_env/lib/python3.9/site-packages/torchtext/data/metrics.py\", line 3, in <module>\n",
      "    import torch\n",
      "  File \"/home/senorita-glez/torch_env/lib/python3.9/site-packages/torch/__init__.py\", line 643, in <module>\n",
      "    from .functional import *  # noqa: F403\n",
      "  File \"/home/senorita-glez/torch_env/lib/python3.9/site-packages/torch/functional.py\", line 6, in <module>\n",
      "    import torch.nn.functional as F\n",
      "  File \"/home/senorita-glez/torch_env/lib/python3.9/site-packages/torch/nn/__init__.py\", line 1, in <module>\n",
      "    from .modules import *  # noqa: F403\n",
      "  File \"/home/senorita-glez/torch_env/lib/python3.9/site-packages/torch/nn/modules/__init__.py\", line 2, in <module>\n",
      "    from .linear import Identity, Linear, Bilinear, LazyLinear\n",
      "  File \"/home/senorita-glez/torch_env/lib/python3.9/site-packages/torch/nn/modules/linear.py\", line 6, in <module>\n",
      "    from .. import functional as F\n",
      "  File \"/home/senorita-glez/torch_env/lib/python3.9/site-packages/torch/nn/functional.py\", line 11, in <module>\n",
      "    from .._jit_internal import boolean_dispatch, _overload, BroadcastingList1, BroadcastingList2, BroadcastingList3\n",
      "  File \"/home/senorita-glez/torch_env/lib/python3.9/site-packages/torch/_jit_internal.py\", line 28, in <module>\n",
      "    import torch.package._mangling as package_mangling\n",
      "  File \"/home/senorita-glez/torch_env/lib/python3.9/site-packages/torch/package/__init__.py\", line 12, in <module>\n",
      "    from .package_importer import PackageImporter\n",
      "  File \"/home/senorita-glez/torch_env/lib/python3.9/site-packages/torch/package/package_importer.py\", line 16, in <module>\n",
      "    from ._directory_reader import DirectoryReader\n",
      "  File \"/home/senorita-glez/torch_env/lib/python3.9/site-packages/torch/package/_directory_reader.py\", line 17, in <module>\n",
      "    _dtype_to_storage = {data_type(0).dtype: data_type for data_type in _storages}\n",
      "  File \"/home/senorita-glez/torch_env/lib/python3.9/site-packages/torch/package/_directory_reader.py\", line 17, in <dictcomp>\n",
      "    _dtype_to_storage = {data_type(0).dtype: data_type for data_type in _storages}\n",
      "/home/senorita-glez/torch_env/lib/python3.9/site-packages/torch/package/_directory_reader.py:17: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at  ../torch/csrc/utils/tensor_numpy.cpp:68.)\n",
      "  _dtype_to_storage = {data_type(0).dtype: data_type for data_type in _storages}\n",
      "/home/senorita-glez/torch_env/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapeo de autores a números:\n",
      "{'adele': np.int64(0), 'blink-182': np.int64(1), 'britney-spears': np.int64(2), 'bruno-mars': np.int64(3), 'lady-gaga': np.int64(4), 'lin-manuel-miranda': np.int64(5), 'michael-jackson': np.int64(6), 'nirvana': np.int64(7), 'radiohead': np.int64(8), 'rihanna': np.int64(9)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/senorita-glez/torch_env/lib/python3.9/site-packages/torchtext/data/utils.py:123: UserWarning: Spacy model \"en\" could not be loaded, trying \"en_core_web_sm\" instead\n",
      "  warnings.warn(f'Spacy model \"{language}\" could not be loaded, trying \"{OLD_MODEL_SHORTCUTS[language]}\" instead')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ejemplo del dataset de entrenamiento: {'Artist_num': '3', 'Lyrics': ['i', \"'d\", 'jump', 'in', 'front', 'of', 'a', 'train', 'for', 'ya', 'you', 'know', 'i', \"'d\", 'do', 'anything', 'for', 'ya', 'i', 'would', 'go', 'through', 'all', 'this', 'pain', 'take', 'a', 'bullet', 'straight', 'right', 'through', 'my', 'brain', 'yes', ',', 'i', 'would', 'die', 'for', 'you', ',', 'baby', 'but', 'you', 'wo', \"n't\", 'do', 'the', 'same', 'no', ',', 'you', 'wo', \"n't\", 'do', 'the', 'same', 'you', 'would', \"n't\", 'do', 'the', 'same', 'ooh', ',', 'you', 'never', 'do', 'the', 'same', 'no', ',', 'no', ',', 'no', ',', 'no', 'this', 'hit', ',', 'that', 'ice', 'cold', 'michelle', 'pfeiffer', ',', 'that', 'white', 'gold', 'this', 'one', 'for', 'them', 'hood', 'girls', 'them', 'good', 'girls', 'straight', 'masterpieces', 'stylin', \"'\", ',', 'whilen', ',', 'livin', \"'\", 'it', 'up', 'in', 'the', 'city', 'got', 'chucks', 'on', 'with', 'saint', 'laurent', 'got', 'kiss', 'myself', ',', 'i', \"'m\", 'so', 'pretty', 'i', \"'m\", 'too', 'hot', '(', 'hot', 'damn', ')', 'called', 'a', 'police', 'and', 'a', 'fireman']}\n",
      "Ejemplo del dataset de validación: {'Artist_num': '6', 'Lyrics': ['no', 'no', 'no', 'no', ',', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', ',', 'no', 'no', 'no', 'no', '[', 'refrain', ']', 'oh', 'baby', ',', 'baby', 'you', 'must', 'be', 'out', 'of', 'your', 'mind', 'do', 'you', 'know', 'what', 'you', \"'re\", 'kicking', 'away', '?', 'we', \"'ve\", 'got', 'a', 'good', 'thing', \"goin'\", ',', 'baby', 'we', \"'ve\", 'got', 'a', 'groovy', 'thing', 'we', \"'ve\", 'got', 'a', 'good', 'thing', \"goin'\", ',', 'baby', 'we', \"'ve\", 'got', 'a', 'groovy', 'thing', 'everybody', \"'s\", 'somebody', \"'s\", 'fool', 'the', 'world', 'is', 'the', 'biggest', 'school', 'as', 'you', 'live', ',', 'you', 'learn', 'though', 'a', 'torch', 'will', 'burn', 'everybody', \"'s\", 'somebody', \"'s\", 'fool', 'you', 'go', 'through', 'life', 'making', 'fools', 'of', 'others']}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torchtext.legacy import data\n",
    "\n",
    "# Leer los datos originales\n",
    "csv_estrofa = \"recolectedLyricsClean2Balanced15.csv\"\n",
    "authorEstrofa = pd.read_csv(csv_estrofa)\n",
    "\n",
    "# Mapear los valores únicos de \"Author\" a números\n",
    "label_encoder = LabelEncoder()\n",
    "authorEstrofa['Artist_num'] = label_encoder.fit_transform(authorEstrofa['Artist'])\n",
    "\n",
    "# Verificar el mapeo\n",
    "print(\"Mapeo de autores a números:\")\n",
    "print(dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_))))\n",
    "\n",
    "# Dividir en conjuntos de entrenamiento y prueba (80-20)\n",
    "estrofa_train, estrofa_test = train_test_split(authorEstrofa, test_size=0.20, random_state=42)\n",
    "\n",
    "# Guardar los conjuntos en archivos CSV temporales con el mapeo reflejado\n",
    "estrofa_train[['Artist_num', 'Lyrics']].to_csv(\"estrofa_train.csv\", index=False)\n",
    "estrofa_test[['Artist_num', 'Lyrics']].to_csv(\"estrofa_test.csv\", index=False)\n",
    "\n",
    "# Definir los campos para torchtext\n",
    "TEXT = data.Field(sequential=True, tokenize=\"spacy\", lower=True, include_lengths=True)\n",
    "SCORE = data.Field(sequential=False, use_vocab=False)  # No se utiliza vocabulario para las etiquetas numéricas\n",
    "datafields = [(\"Artist_num\", SCORE), (\"Lyrics\", TEXT)]  # Ajusta los nombres de las columnas según tu CSV\n",
    "\n",
    "# Crear los datasets de torchtext a partir de los archivos CSV\n",
    "train = data.TabularDataset(\n",
    "    path=\"estrofa_train.csv\", \n",
    "    format=\"csv\", \n",
    "    fields=datafields, \n",
    "    skip_header=True\n",
    ")\n",
    "\n",
    "val = data.TabularDataset(\n",
    "    path=\"estrofa_test.csv\", \n",
    "    format=\"csv\", \n",
    "    fields=datafields, \n",
    "    skip_header=True\n",
    ")\n",
    "\n",
    "# Imprimir información para verificar\n",
    "print(f\"Ejemplo del dataset de entrenamiento: {vars(train.examples[0])}\")\n",
    "print(f\"Ejemplo del dataset de validación: {vars(val.examples[0])}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/senorita-glez/torch_env/lib/python3.9/site-packages/torchtext/data/utils.py:123: UserWarning: Spacy model \"en\" could not be loaded, trying \"en_core_web_sm\" instead\n",
      "  warnings.warn(f'Spacy model \"{language}\" could not be loaded, trying \"{OLD_MODEL_SHORTCUTS[language]}\" instead')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paso 1: Dimensiones de embedded: torch.Size([122, 16, 100])\n",
      "Paso 1: Longitudes originales: tensor([122, 122, 122, 122, 122, 122, 122, 122, 121, 121, 121, 121, 121, 120,\n",
      "        120, 119], device='cuda:0')\n",
      "Paso 2: Dimensiones de embedded después de ordenar: torch.Size([122, 16, 100])\n",
      "Paso 2: Longitudes después de ordenar: tensor([122, 122, 122, 122, 122, 122, 122, 122, 121, 121, 121, 121, 121, 120,\n",
      "        120, 119], device='cuda:0')\n",
      "Paso 3: Empaquetado exitoso.\n",
      "Paso 4: Salida de la red recurrente obtenida.\n",
      "Paso 5: Dimensiones de hidden: torch.Size([16, 300])\n",
      "Paso 6: Dimensiones de output: torch.Size([16, 10])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Numpy is not available",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 228\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;66;03m# Entrenar y evaluar el modelo\u001b[39;00m\n\u001b[1;32m    227\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[0;32m--> 228\u001b[0m \u001b[43mtrain_classifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_iterator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;66;03m# Evaluación\u001b[39;00m\n\u001b[1;32m    231\u001b[0m preds, labels \u001b[38;5;241m=\u001b[39m evaluate_classifier(model, val_iterator, loss_function)\n",
      "Cell \u001b[0;32mIn[2], line 183\u001b[0m, in \u001b[0;36mtrain_classifier\u001b[0;34m(model, dataset_iterator, loss_function, optimizer, num_epochs, log)\u001b[0m\n\u001b[1;32m    181\u001b[0m     total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m    182\u001b[0m     total_acc \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (pred \u001b[38;5;241m==\u001b[39m labels)\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m--> 183\u001b[0m     total_f1 \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mf1_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpred\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maverage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmacro\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    185\u001b[0m writer\u001b[38;5;241m.\u001b[39madd_scalar(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoss/train\u001b[39m\u001b[38;5;124m\"\u001b[39m, total_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(dataset_iterator), epoch)\n\u001b[1;32m    186\u001b[0m writer\u001b[38;5;241m.\u001b[39madd_scalar(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccuracy/train\u001b[39m\u001b[38;5;124m\"\u001b[39m, total_acc \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(dataset_iterator\u001b[38;5;241m.\u001b[39mdataset), epoch)\n",
      "File \u001b[0;32m~/torch_env/lib/python3.9/site-packages/sklearn/utils/_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    211\u001b[0m         )\n\u001b[1;32m    212\u001b[0m     ):\n\u001b[0;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    223\u001b[0m     )\n",
      "File \u001b[0;32m~/torch_env/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1293\u001b[0m, in \u001b[0;36mf1_score\u001b[0;34m(y_true, y_pred, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m   1113\u001b[0m \u001b[38;5;129m@validate_params\u001b[39m(\n\u001b[1;32m   1114\u001b[0m     {\n\u001b[1;32m   1115\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray-like\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msparse matrix\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1140\u001b[0m     zero_division\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwarn\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1141\u001b[0m ):\n\u001b[1;32m   1142\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute the F1 score, also known as balanced F-score or F-measure.\u001b[39;00m\n\u001b[1;32m   1143\u001b[0m \n\u001b[1;32m   1144\u001b[0m \u001b[38;5;124;03m    The F1 score can be interpreted as a harmonic mean of the precision and\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1291\u001b[0m \u001b[38;5;124;03m    array([0.66666667, 1.        , 0.66666667])\u001b[39;00m\n\u001b[1;32m   1292\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfbeta_score\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1294\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1295\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1296\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1297\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1298\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpos_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpos_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1299\u001b[0m \u001b[43m        \u001b[49m\u001b[43maverage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maverage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1300\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1301\u001b[0m \u001b[43m        \u001b[49m\u001b[43mzero_division\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mzero_division\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1302\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/torch_env/lib/python3.9/site-packages/sklearn/utils/_param_validation.py:186\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    184\u001b[0m global_skip_validation \u001b[38;5;241m=\u001b[39m get_config()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskip_parameter_validation\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m global_skip_validation:\n\u001b[0;32m--> 186\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    188\u001b[0m func_sig \u001b[38;5;241m=\u001b[39m signature(func)\n\u001b[1;32m    190\u001b[0m \u001b[38;5;66;03m# Map *args/**kwargs to the function signature\u001b[39;00m\n",
      "File \u001b[0;32m~/torch_env/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1485\u001b[0m, in \u001b[0;36mfbeta_score\u001b[0;34m(y_true, y_pred, beta, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m   1305\u001b[0m \u001b[38;5;129m@validate_params\u001b[39m(\n\u001b[1;32m   1306\u001b[0m     {\n\u001b[1;32m   1307\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray-like\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msparse matrix\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1334\u001b[0m     zero_division\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwarn\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1335\u001b[0m ):\n\u001b[1;32m   1336\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute the F-beta score.\u001b[39;00m\n\u001b[1;32m   1337\u001b[0m \n\u001b[1;32m   1338\u001b[0m \u001b[38;5;124;03m    The F-beta score is the weighted harmonic mean of precision and recall,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1482\u001b[0m \u001b[38;5;124;03m    np.float64(0.12...)\u001b[39;00m\n\u001b[1;32m   1483\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1485\u001b[0m     _, _, f, _ \u001b[38;5;241m=\u001b[39m \u001b[43mprecision_recall_fscore_support\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1486\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1487\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1488\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1489\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1490\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpos_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpos_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1491\u001b[0m \u001b[43m        \u001b[49m\u001b[43maverage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maverage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1492\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwarn_for\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mf-score\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1493\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mzero_division\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mzero_division\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1495\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1496\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f\n",
      "File \u001b[0;32m~/torch_env/lib/python3.9/site-packages/sklearn/utils/_param_validation.py:186\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    184\u001b[0m global_skip_validation \u001b[38;5;241m=\u001b[39m get_config()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskip_parameter_validation\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m global_skip_validation:\n\u001b[0;32m--> 186\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    188\u001b[0m func_sig \u001b[38;5;241m=\u001b[39m signature(func)\n\u001b[1;32m    190\u001b[0m \u001b[38;5;66;03m# Map *args/**kwargs to the function signature\u001b[39;00m\n",
      "File \u001b[0;32m~/torch_env/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1789\u001b[0m, in \u001b[0;36mprecision_recall_fscore_support\u001b[0;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m   1626\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Compute precision, recall, F-measure and support for each class.\u001b[39;00m\n\u001b[1;32m   1627\u001b[0m \n\u001b[1;32m   1628\u001b[0m \u001b[38;5;124;03mThe precision is the ratio ``tp / (tp + fp)`` where ``tp`` is the number of\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1786\u001b[0m \u001b[38;5;124;03m array([2, 2, 2]))\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1788\u001b[0m _check_zero_division(zero_division)\n\u001b[0;32m-> 1789\u001b[0m labels \u001b[38;5;241m=\u001b[39m \u001b[43m_check_set_wise_labels\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maverage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos_label\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1791\u001b[0m \u001b[38;5;66;03m# Calculate tp_sum, pred_sum, true_sum ###\u001b[39;00m\n\u001b[1;32m   1792\u001b[0m samplewise \u001b[38;5;241m=\u001b[39m average \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msamples\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/torch_env/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1561\u001b[0m, in \u001b[0;36m_check_set_wise_labels\u001b[0;34m(y_true, y_pred, average, labels, pos_label)\u001b[0m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m average \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m average_options \u001b[38;5;129;01mand\u001b[39;00m average \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1559\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maverage has to be one of \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(average_options))\n\u001b[0;32m-> 1561\u001b[0m y_type, y_true, y_pred \u001b[38;5;241m=\u001b[39m \u001b[43m_check_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1562\u001b[0m \u001b[38;5;66;03m# Convert to Python primitive type to avoid NumPy type / Python str\u001b[39;00m\n\u001b[1;32m   1563\u001b[0m \u001b[38;5;66;03m# comparison. See https://github.com/numpy/numpy/issues/6784\u001b[39;00m\n\u001b[1;32m   1564\u001b[0m present_labels \u001b[38;5;241m=\u001b[39m unique_labels(y_true, y_pred)\u001b[38;5;241m.\u001b[39mtolist()\n",
      "File \u001b[0;32m~/torch_env/lib/python3.9/site-packages/sklearn/metrics/_classification.py:104\u001b[0m, in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m    102\u001b[0m xp, _ \u001b[38;5;241m=\u001b[39m get_namespace(y_true, y_pred)\n\u001b[1;32m    103\u001b[0m check_consistent_length(y_true, y_pred)\n\u001b[0;32m--> 104\u001b[0m type_true \u001b[38;5;241m=\u001b[39m \u001b[43mtype_of_target\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43my_true\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    105\u001b[0m type_pred \u001b[38;5;241m=\u001b[39m type_of_target(y_pred, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_pred\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    107\u001b[0m y_type \u001b[38;5;241m=\u001b[39m {type_true, type_pred}\n",
      "File \u001b[0;32m~/torch_env/lib/python3.9/site-packages/sklearn/utils/multiclass.py:314\u001b[0m, in \u001b[0;36mtype_of_target\u001b[0;34m(y, input_name)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sparse_pandas:\n\u001b[1;32m    312\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my cannot be class \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSparseSeries\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSparseArray\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 314\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mis_multilabel\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    315\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultilabel-indicator\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;66;03m# DeprecationWarning will be replaced by ValueError, see NEP 34\u001b[39;00m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;66;03m# https://numpy.org/neps/nep-0034-infer-dtype-is-object.html\u001b[39;00m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;66;03m# We therefore catch both deprecation (NumPy < 1.24) warning and\u001b[39;00m\n\u001b[1;32m    320\u001b[0m \u001b[38;5;66;03m# value error (NumPy >= 1.24).\u001b[39;00m\n",
      "File \u001b[0;32m~/torch_env/lib/python3.9/site-packages/sklearn/utils/multiclass.py:169\u001b[0m, in \u001b[0;36mis_multilabel\u001b[0;34m(y)\u001b[0m\n\u001b[1;32m    167\u001b[0m warnings\u001b[38;5;241m.\u001b[39msimplefilter(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m, VisibleDeprecationWarning)\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 169\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_y_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (VisibleDeprecationWarning, \u001b[38;5;167;01mValueError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComplex data not supported\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/torch_env/lib/python3.9/site-packages/sklearn/utils/validation.py:1012\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m   1010\u001b[0m         array \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39mastype(array, dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   1011\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1012\u001b[0m         array \u001b[38;5;241m=\u001b[39m \u001b[43m_asarray_with_order\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1013\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ComplexWarning \u001b[38;5;28;01mas\u001b[39;00m complex_warning:\n\u001b[1;32m   1014\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1015\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComplex data not supported\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(array)\n\u001b[1;32m   1016\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcomplex_warning\u001b[39;00m\n",
      "File \u001b[0;32m~/torch_env/lib/python3.9/site-packages/sklearn/utils/_array_api.py:745\u001b[0m, in \u001b[0;36m_asarray_with_order\u001b[0;34m(array, dtype, order, copy, xp, device)\u001b[0m\n\u001b[1;32m    743\u001b[0m     array \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39marray(array, order\u001b[38;5;241m=\u001b[39morder, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m    744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 745\u001b[0m     array \u001b[38;5;241m=\u001b[39m \u001b[43mnumpy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;66;03m# At this point array is a NumPy ndarray. We convert it to an array\u001b[39;00m\n\u001b[1;32m    748\u001b[0m \u001b[38;5;66;03m# container that is consistent with the input's namespace.\u001b[39;00m\n\u001b[1;32m    749\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m xp\u001b[38;5;241m.\u001b[39masarray(array)\n",
      "File \u001b[0;32m~/torch_env/lib/python3.9/site-packages/torch/_tensor.py:678\u001b[0m, in \u001b[0;36mTensor.__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(Tensor\u001b[38;5;241m.\u001b[39m__array__, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m    677\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 678\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    680\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Numpy is not available"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchtext.legacy import data\n",
    "from torchtext.vocab import GloVe\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import f1_score\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "#torch.backends.cudnn.enabled = False\n",
    "\n",
    "\n",
    "# Leer los datos originales\n",
    "csv_estrofa = \"recolectedLyricsClean2Balanced15.csv\"\n",
    "authorEstrofa = pd.read_csv(csv_estrofa)\n",
    "\n",
    "# Mapear los valores únicos de \"Artist\" a números\n",
    "label_encoder = LabelEncoder()\n",
    "authorEstrofa['Artist_num'] = label_encoder.fit_transform(authorEstrofa['Artist'])\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "estrofa_train, estrofa_test = train_test_split(authorEstrofa, test_size=0.20, random_state=42)\n",
    "\n",
    "# Guardar los conjuntos en archivos CSV temporales\n",
    "estrofa_train[['Artist_num', 'Lyrics']].to_csv(\"estrofa_train.csv\", index=False)\n",
    "estrofa_test[['Artist_num', 'Lyrics']].to_csv(\"estrofa_test.csv\", index=False)\n",
    "\n",
    "# Definir los campos para TorchText\n",
    "TEXT = data.Field(sequential=True, tokenize=\"spacy\", lower=True, include_lengths=True)\n",
    "LABEL = data.LabelField(sequential=False, use_vocab=True)\n",
    "\n",
    "datafields = [(\"Artist_num\", LABEL), (\"Lyrics\", TEXT)]\n",
    "\n",
    "# Crear datasets\n",
    "train, val = data.TabularDataset.splits(\n",
    "    path=\".\",\n",
    "    train=\"estrofa_train.csv\",\n",
    "    validation=\"estrofa_test.csv\",\n",
    "    format=\"csv\",\n",
    "    fields=datafields,\n",
    "    skip_header=True\n",
    ")\n",
    "\n",
    "# Construir vocabulario\n",
    "TEXT.build_vocab(train, val, min_freq=3, vectors=GloVe(name=\"6B\", dim=100))\n",
    "LABEL.build_vocab(train)\n",
    "\n",
    "# Crear iteradores para los datos\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "train_iterator = data.BucketIterator(\n",
    "    train, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    sort_key=lambda x: len(x.Lyrics), \n",
    "    sort_within_batch=True, \n",
    "    device=device\n",
    ")\n",
    "\n",
    "val_iterator = data.BucketIterator(\n",
    "    val, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    sort_key=lambda x: len(x.Lyrics),\n",
    "    sort_within_batch=True,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Definir la clase del modelo\n",
    "class AuthorClassifier(nn.Module):\n",
    "    def __init__(self, mode, output_size, hidden_size, vocab_size, embedding_length, word_embeddings):\n",
    "        super(AuthorClassifier, self).__init__()\n",
    "        \n",
    "        # Validar el modo seleccionado\n",
    "        if mode not in ['rnn', 'lstm', 'gru', 'bilstm']:\n",
    "            raise ValueError(\"El modo debe ser uno de: 'rnn', 'lstm', 'gru', 'bilstm'\")\n",
    "\n",
    "        self.mode = mode\n",
    "\n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_length)\n",
    "        self.embedding.weight = nn.Parameter(word_embeddings, requires_grad=False)\n",
    "\n",
    "        # Definir la red recurrente según el modo\n",
    "        if mode == 'rnn':\n",
    "            self.network = nn.RNN(embedding_length, hidden_size, batch_first=True)\n",
    "        elif mode == 'gru':\n",
    "            self.network = nn.GRU(embedding_length, hidden_size, batch_first=True)\n",
    "        elif mode in ['lstm', 'bilstm']:\n",
    "            self.network = nn.LSTM(\n",
    "                embedding_length, \n",
    "                hidden_size, \n",
    "                batch_first=True, \n",
    "                bidirectional=(mode == 'bilstm')\n",
    "            )\n",
    "\n",
    "        # Fully connected layer para la salida\n",
    "        self.fc = nn.Linear(hidden_size * 2 if mode == 'bilstm' else hidden_size, output_size)\n",
    "\n",
    "    def forward(self, text, text_lengths):\n",
    "        # Paso 1: Convertir a embeddings\n",
    "        embedded = self.embedding(text)\n",
    "        print(\"Paso 1: Dimensiones de embedded:\", embedded.shape)\n",
    "        print(\"Paso 1: Longitudes originales:\", text_lengths)\n",
    "\n",
    "        # Paso 2: Ordenar las secuencias manualmente\n",
    "        try:\n",
    "            sorted_lengths, perm_idx = text_lengths.sort(0, descending=True)\n",
    "            embedded = embedded[:, perm_idx, :]  # Reordenar a lo largo del eje \"batches\"\n",
    "            print(\"Paso 2: Dimensiones de embedded después de ordenar:\", embedded.shape)\n",
    "            print(\"Paso 2: Longitudes después de ordenar:\", sorted_lengths)\n",
    "        except Exception as e:\n",
    "            print(\"Error en ordenamiento de secuencias:\", e)\n",
    "            raise e\n",
    "\n",
    "        # Paso 3: Empaquetar las secuencias\n",
    "        try:\n",
    "            packed_embedded = nn.utils.rnn.pack_padded_sequence(\n",
    "                embedded, \n",
    "                sorted_lengths.cpu(), \n",
    "                batch_first=False  # batch_first=False porque tenemos [time_steps, batch_size, embedding_dim]\n",
    "            )\n",
    "            print(\"Paso 3: Empaquetado exitoso.\")\n",
    "        except Exception as e:\n",
    "            print(\"Error durante el empaquetado:\", e)\n",
    "            raise e\n",
    "\n",
    "        # Paso 4: Pasar por la red recurrente\n",
    "        try:\n",
    "            if isinstance(self.network, nn.LSTM):\n",
    "                packed_output, (hidden, _) = self.network(packed_embedded)\n",
    "            else:\n",
    "                packed_output, hidden = self.network(packed_embedded)\n",
    "            print(\"Paso 4: Salida de la red recurrente obtenida.\")\n",
    "        except Exception as e:\n",
    "            print(\"Error al pasar por la red recurrente:\", e)\n",
    "            raise e\n",
    "\n",
    "        # Paso 5: Procesar la capa oculta\n",
    "        try:\n",
    "            if self.mode == 'bilstm':\n",
    "                hidden = torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim=1)\n",
    "            else:\n",
    "                hidden = hidden[-1, :, :]\n",
    "            print(\"Paso 5: Dimensiones de hidden:\", hidden.shape)\n",
    "        except Exception as e:\n",
    "            print(\"Error al procesar la capa oculta:\", e)\n",
    "            raise e\n",
    "\n",
    "        # Paso 6: Pasar por la capa completamente conectada\n",
    "        try:\n",
    "            output = self.fc(hidden)\n",
    "            print(\"Paso 6: Dimensiones de output:\", output.shape)\n",
    "            return output\n",
    "        except Exception as e:\n",
    "            print(\"Error en la capa completamente conectada:\", e)\n",
    "        raise e\n",
    "\n",
    "\n",
    "\n",
    "# Entrenamiento y evaluación\n",
    "def train_classifier(model, dataset_iterator, loss_function, optimizer, num_epochs, log=\"runs\"):\n",
    "    writer = SummaryWriter(log_dir=log)\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss, total_acc, total_f1 = 0, 0, 0\n",
    "        for batch in dataset_iterator:\n",
    "            text, text_lengths = batch.Lyrics\n",
    "            labels = batch.Artist_num\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(text, text_lengths).squeeze(0)\n",
    "            loss = loss_function(output, labels.long())\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "            optimizer.step()\n",
    "\n",
    "            pred = output.argmax(1)\n",
    "            total_loss += loss.item()\n",
    "            total_acc += (pred == labels).sum().item()\n",
    "            total_f1 += f1_score(labels.cpu(), pred.cpu(), average='macro')\n",
    "\n",
    "        writer.add_scalar(\"Loss/train\", total_loss / len(dataset_iterator), epoch)\n",
    "        writer.add_scalar(\"Accuracy/train\", total_acc / len(dataset_iterator.dataset), epoch)\n",
    "        writer.add_scalar(\"F1 Score/train\", total_f1 / len(dataset_iterator), epoch)\n",
    "        print(f\"Epoch {epoch+1}: Loss={total_loss:.4f}, Accuracy={total_acc/len(dataset_iterator.dataset):.4f}, F1={total_f1/len(dataset_iterator):.4f}\")\n",
    "\n",
    "def evaluate_classifier(model, dataset_iterator, loss_function):\n",
    "    model.eval()\n",
    "    total_loss, total_acc, total_f1 = 0, 0, 0\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in dataset_iterator:\n",
    "            text, text_lengths = batch.Lyrics\n",
    "            labels = batch.Artist_num\n",
    "            output = model(text, text_lengths).squeeze(0)\n",
    "            loss = loss_function(output, labels.long())\n",
    "            pred = output.argmax(1)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_acc += (pred == labels).sum().item()\n",
    "            total_f1 += f1_score(labels.cpu(), pred.cpu(), average='macro')\n",
    "            all_preds.extend(pred.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    print(f\"Validation: Loss={total_loss:.4f}, Accuracy={total_acc/len(dataset_iterator.dataset):.4f}, F1={total_f1/len(dataset_iterator):.4f}\")\n",
    "    return np.array(all_preds), np.array(all_labels)\n",
    "\n",
    "# Configuración del modelo\n",
    "output_size = len(LABEL.vocab)\n",
    "hidden_size = 300\n",
    "vocab_size = len(TEXT.vocab)\n",
    "embedding_length = 100\n",
    "word_embeddings = TEXT.vocab.vectors\n",
    "\n",
    "# Probar diferentes arquitecturas\n",
    "mode = 'lstm'  # Cambiar a 'rnn', 'gru', o 'bilstm' para probar otros métodos\n",
    "model = AuthorClassifier(mode, output_size, hidden_size, vocab_size, embedding_length, word_embeddings)\n",
    "model = model.to(device)\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Entrenar y evaluar el modelo\n",
    "num_epochs = 10\n",
    "train_classifier(model, train_iterator, loss_function, optimizer, num_epochs)\n",
    "\n",
    "# Evaluación\n",
    "preds, labels = evaluate_classifier(model, val_iterator, loss_function)\n",
    "\n",
    "# Matriz de confusión\n",
    "confusion_matrix = np.zeros((output_size, output_size), dtype=int)\n",
    "for t, p in zip(labels, preds):\n",
    "    confusion_matrix[t, p] += 1\n",
    "\n",
    "plt.figure(figsize=(15, 15))\n",
    "sns.heatmap(confusion_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=LABEL.vocab.itos, yticklabels=LABEL.vocab.itos)\n",
    "plt.xlabel('Predicción')\n",
    "plt.ylabel('Etiqueta Real')\n",
    "plt.title('Matriz de Confusión')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (torch_env)",
   "language": "python",
   "name": "torch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
